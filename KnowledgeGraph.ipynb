{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esusrutha/zeroshot/blob/main/KnowledgeGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWVPwUWc1YoD",
        "outputId": "cd57cc75-f6cb-405f-fd72-6db808892cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx82kpEu1Ftw",
        "outputId": "44b83ef4-61b1-4875-8591-bf5df5fbd92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.0.tar.gz (616 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.0-py3-none-any.whl size=909897 sha256=6dacda606177da04499b6ef63b5b5a5ef285fd6c42ca1cf27cf7842ce3334001\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/7d/6b/17150450b80b4a3656a84330e22709ccd8dc0f8f4773ba4133\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zHZn5GQ1zrM",
        "outputId": "a87cf395-710e-47d9-80c9-749aaac48eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=91066025048cd3c7fda06b6e72dc150c24eca1884e2bef06a4d7df9f6034bf13\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.14.1 sentence-transformers-2.2.2 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSyZtDbrOh78",
        "outputId": "994c3e53-2d4f-4e67-e515-9ef177add470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/100, Loss: 0.18791426718235016\n",
            "tensor([[ 0.1262, -0.2471,  0.7686,  ..., -0.0542,  0.0829, -0.0556],\n",
            "        [ 0.1262, -0.2471,  0.7686,  ..., -0.0542,  0.0829, -0.0556],\n",
            "        [ 0.1262, -0.2471,  0.7686,  ..., -0.0542,  0.0829, -0.0556],\n",
            "        ...,\n",
            "        [ 0.1262, -0.2471,  0.7686,  ..., -0.0542,  0.0829, -0.0556],\n",
            "        [ 0.1262, -0.2471,  0.7686,  ..., -0.0542,  0.0829, -0.0556],\n",
            "        [ 0.1262, -0.2471,  0.7686,  ..., -0.0542,  0.0829, -0.0556]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#Fixed code for S-BERT-KG embedding alignment\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "\n",
        "#Load the data from two csv files and merge them into a single dataframe\n",
        "RESULTS_DIR = 'drive/MyDrive'\n",
        "tweets_df1 = pd.read_csv(RESULTS_DIR + '/D1.csv', encoding='latin1', engine='python')\n",
        "tweets_df2 = pd.read_csv(RESULTS_DIR + '/D22.csv', encoding='latin1', engine='python')\n",
        "df = pd.concat([tweets_df1, tweets_df2], ignore_index=True)\n",
        "\n",
        "#Load pre-trained S-BERT model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "#Define a function to embed a text using S-BERT\n",
        "def embed_text(text):\n",
        "  # Embed the text using S-BERT\n",
        "  if isinstance(text, str):\n",
        "      embeddings = model.encode(text)\n",
        "  else:\n",
        "    embeddings = model.encode([t for t in text if isinstance(t, str)])\n",
        "    # Convert the embeddings to a PyTorch tensor\n",
        "    tensor = torch.FloatTensor(embeddings)\n",
        "  # Return the tensor\n",
        "  return tensor\n",
        "\n",
        "#Define a function to create a PyTorch Geometric graph from a knowledge graph\n",
        "def create_kg_graph(kg):\n",
        "  # Get the nodes and edges from the knowledge graph\n",
        "  nodes = list(kg.keys())\n",
        "  edges = []\n",
        "  for node1 in kg.keys():\n",
        "    for node2 in kg[node1]:\n",
        "      if isinstance(node2, list):\n",
        "        node2 = ', '.join(node2)\n",
        "      if node1 in nodes and node2 in nodes:\n",
        "        edges.append((nodes.index(node1), nodes.index(node2)))\n",
        "        # Create the PyTorch Geometric graph data object\n",
        "        data = Data(x=torch.eye(len(nodes)), edge_index=torch.LongTensor(edges).t())\n",
        "  # Return the graph data object\n",
        "  return data\n",
        "\n",
        "#Define a knowledge graph\n",
        "kg = {\n",
        "'tweet': ['Advice','China','Mask','News','Transportation','USA','Vaccine'],\n",
        "'Advice': ['Stay at home','wash hands','wear mask','social distancing'],\n",
        "'China': ['Wuhan','China Coronavirus Updates','China news','other tweets related to China'],\n",
        "'Mask': ['Mask shortage','wear mask','mask types','N50','N95','3M8210','3M9001','3M9322','3M9501'],\n",
        "'News': ['Coronavirus updates','news','rules'],\n",
        "'Transportation': ['Flights','traffic','traveling'],\n",
        "'USA': ['U.S. Coronavirus Updates','COVID19','U.S. news','United States','US','USA'],\n",
        "'Vaccine': ['Vaccine news','vaccine progress','vaccine injection'],\n",
        "}\n",
        "def create_kg_graph(kg, embeddings):\n",
        "  # Get the nodes and edges from the knowledge graph\n",
        "  nodes = list(kg.keys())\n",
        "  edges = []\n",
        "  for node1 in kg.keys():\n",
        "    for node2 in kg[node1]:\n",
        "      if isinstance(node2, list):\n",
        "        node2 = ', '.join(node2)\n",
        "      if node1 in nodes and node2 in nodes:\n",
        "        edges.append((nodes.index(node1), nodes.index(node2)))\n",
        "  # Create the PyTorch Geometric graph data object\n",
        "  x = torch.FloatTensor(embeddings)\n",
        "  data = Data(x=x, edge_index=torch.LongTensor(edges).t())\n",
        "  return data\n",
        "#Embed the text and create the PyTorch Geometric graph data object\n",
        "text = df[\"hashtags\"].apply(lambda x: ', '.join(x.strip(\"[]\").split(\", \")) if isinstance(x, str) else '')\n",
        "embeddings = embed_text(text)\n",
        "graph_data = create_kg_graph(kg, embeddings)\n",
        "\n",
        "\n",
        "#Define a GCN model\n",
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(GCN, self).__init__()\n",
        "    self.conv1 = GCNConv(in_channels=in_channels, out_channels=out_channels)\n",
        "    self.conv2 = GCNConv(in_channels=out_channels, out_channels=out_channels)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    return x\n",
        "   \n",
        "#Define a function  to train the GCN model\n",
        "def train_gcn(model, data, embeddings, epochs):\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "  criterion = torch.nn.MSELoss()\n",
        "  for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data.x, data.edge_index)\n",
        "    loss = criterion(output, embeddings)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "#Embed the text and create the PyTorch Geometric graph data object\n",
        "text = df[\"hashtags\"].apply(lambda x: ', '.join(x.strip(\"[]\").split(\", \")) if isinstance(x, str) else '')\n",
        "\n",
        "embeddings = embed_text(text)\n",
        "\n",
        "#Reshape the embeddings tensor to have shape (num_tweets, embedding_size)\n",
        "\n",
        "embeddings = embeddings.reshape(len(text), embeddings.shape[1])\n",
        "\n",
        "graph_data = create_kg_graph(kg, embeddings)\n",
        "\n",
        "#Train the GCN model\n",
        "model = GCN(embeddings.shape[1], embeddings.shape[1])\n",
        "train_gcn(model, graph_data, embeddings, 100)\n",
        "\n",
        "#Get the aligned embeddings\n",
        "aligned_embeddings = model(graph_data.x, graph_data.edge_index)\n",
        "print(aligned_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6572qOSVL5i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VKMNnjgPtoFTsAwkAO6q68xc30ZJfQIE",
      "authorship_tag": "ABX9TyN7h8N2PgW8+WyioXqSqj/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}